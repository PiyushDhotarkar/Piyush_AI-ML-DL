{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credits @https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## Importing the data from tensorflow itself.\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining some network parameters\n",
    "hidden_1 = 512\n",
    "hidden_2 = 128\n",
    "input_size = 784\n",
    "classes = 10\n",
    "\n",
    "## Defining the placeholders\\\n",
    "x = tf.placeholder(tf.float32, [None, 784])    ## Placeholders for x's.\n",
    "y_act = tf.placeholder(tf.float32, [None, 10]) ## Placeholders for y's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Initialization Using two different Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights initialization\n",
    "\n",
    "## Initialization of weights for sigmoid units using Xavier/Glorot initialization technique.\n",
    "weights_sig = {\n",
    "    'h1':tf.Variable(tf.random_normal([input_size, hidden_1],stddev=0.039, mean=0)), ## 84x512 # sqrt(2/(784+512)) = 0.039\n",
    "    'h2':tf.Variable(tf.random_normal([hidden_1, hidden_2],stddev=0.055, mean=0)),   ##512x128 # sqrt(2/(512+128)) = 0.055\n",
    "    'out':tf.Variable(tf.random_normal([hidden_2, classes],stddev=0.120, mean=0))   ##128x10\n",
    "}\n",
    "\n",
    "## Initialization of weights for relu units using HE initialization technique.\n",
    "weights_relu = {\n",
    "    'h1':tf.Variable(tf.random_normal([input_size, hidden_1],stddev=0.062, mean=0)), ##784*512\n",
    "    'h2':tf.Variable(tf.random_normal([hidden_1, hidden_2],stddev=0.125, mean=0)),   ##512*128\n",
    "    'out':tf.Variable(tf.random_normal([hidden_2, classes],stddev=0.120, mean=0))   ##128*10\n",
    "}\n",
    "\n",
    "## Addition of biases.\n",
    "biases = {\n",
    "    'b1':tf.Variable(tf.random_normal([hidden_1])),    ##512*1\n",
    "    'b2':tf.Variable(tf.random_normal([hidden_2])),    ##128*1\n",
    "    'out':tf.Variable(tf.random_normal([classes]))     ##10*1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainng Parameters\n",
    "training_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model 1 with Sigmoidal Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model 1 with Sigmoidal activation functions.\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \n",
    "    ## printing some of the shapes, weights of the data\n",
    "    print('x',x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b1:',biases['b1'].get_shape())\n",
    "    \n",
    "    ## Defining the 1st layer with sigmoid activation\n",
    "    layer1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer1 = tf.nn.sigmoid(layer1)\n",
    "    print('layer_1:',layer1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b2:',biases['b2'].get_shape())\n",
    "    \n",
    "    ## defining the 2nd layer with sigmoid activation\n",
    "    layer2 = tf.add(tf.matmul(layer1, weights['h2']), biases['b2'])\n",
    "    layer2 = tf.nn.sigmoid(layer2)\n",
    "    print('layer_1:',layer2.get_shape(), 'W[h2]:', weights['out'].get_shape(), 'b3:',biases['out'].get_shape())\n",
    "    \n",
    "    ## Output layer with sigmoid activation\n",
    "    out_layer = tf.matmul(layer2, weights['out']) + biases['out']     \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 784) W[h1]: (784, 512) b1: (512,)\n",
      "layer_1: (?, 512) W[h2]: (512, 128) b2: (128,)\n",
      "layer_1: (?, 128) W[h2]: (128, 10) b3: (10,)\n",
      "out_layer: (?, 10)\n",
      "Epoch: 0001 train cost=1.686316505 test cost=1.679869107\n",
      "Accuracy: 0.9226\n",
      "Epoch: 0002 train cost=1.536799784 test cost=1.534815071\n",
      "Accuracy: 0.9394\n",
      "Epoch: 0003 train cost=1.514831131 test cost=1.515243659\n",
      "Accuracy: 0.9497\n",
      "Epoch: 0004 train cost=1.503162249 test cost=1.504582683\n",
      "Accuracy: 0.9575\n",
      "Epoch: 0005 train cost=1.494824588 test cost=1.497273717\n",
      "Accuracy: 0.9631\n",
      "Epoch: 0006 train cost=1.488420636 test cost=1.492361705\n",
      "Accuracy: 0.9674\n",
      "Epoch: 0007 train cost=1.484014179 test cost=1.489046976\n",
      "Accuracy: 0.9711\n",
      "Epoch: 0008 train cost=1.480320408 test cost=1.486407732\n",
      "Accuracy: 0.9727\n",
      "Epoch: 0009 train cost=1.477567188 test cost=1.484454814\n",
      "Accuracy: 0.9754\n",
      "Epoch: 0010 train cost=1.475278144 test cost=1.483601665\n",
      "Accuracy: 0.9757\n",
      "Epoch: 0011 train cost=1.473371278 test cost=1.482388913\n",
      "Accuracy: 0.9763\n",
      "Epoch: 0012 train cost=1.471868505 test cost=1.481928586\n",
      "Accuracy: 0.9767\n",
      "Epoch: 0013 train cost=1.470588816 test cost=1.481532895\n",
      "Accuracy: 0.9785\n",
      "Epoch: 0014 train cost=1.469269415 test cost=1.481030892\n",
      "Accuracy: 0.9774\n",
      "Epoch: 0015 train cost=1.468361217 test cost=1.480934444\n",
      "Accuracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "## Deifing the optimizers and cost function and actually running the model\n",
    "pred = multilayer_perceptron(x, weights_sig, biases)\n",
    "\n",
    "## defining the loss function\n",
    "cost_fn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y_act))\n",
    "\n",
    "## Optimizers\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_fn)\n",
    "\n",
    "## Running the model\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(data.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "            \n",
    "            _, c, w = sess.run([adam, cost_fn, weights_sig], feed_dict={x:batch_xs, y_act: batch_ys})\n",
    "            train_avg_cost = train_avg_cost + c / total_batch\n",
    "            c = sess.run(cost_fn, feed_dict={x: data.test.images, y_act:data.test.labels})\n",
    "            test_avg_cost = test_avg_cost + c / total_batch\n",
    "            \n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        \n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "        \n",
    "        ## Prediction on the test data\n",
    "        predictions = tf.equal(tf.argmax(pred,1), tf.argmax(y_act,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "        print(\"Accuracy:\", accuracy.eval({x: data.test.images, y_act: data.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model 2 with Relu Activation Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Relu model\n",
    "def multilayer_perceptron_relu(x, weights, biases):\n",
    "\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())        \n",
    "    layer1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) \n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    print( 'layer_1:', layer1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b2', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer2 = tf.add(tf.matmul(layer1, weights['h2']), biases['b2']) \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    print( 'layer_2:', layer2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer2, weights['out']) + biases['out']   \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (?, 784) W[h1]: (784, 512) b1: (512,)\n",
      "layer_1: (?, 512) W[h2]: (512, 128) b2 (128,)\n",
      "layer_2: (?, 128) W[out]: (128, 10) b3: (10,)\n",
      "out_layer: (?, 10)\n",
      "Epoch: 0001 train cost=1.557785965 test cost=1.551697966\n",
      "Epoch: 0002 train cost=1.499787403 test cost=1.499863870\n",
      "Epoch: 0003 train cost=1.488317022 test cost=1.491868765\n",
      "Epoch: 0004 train cost=1.482088075 test cost=1.487862669\n",
      "Epoch: 0005 train cost=1.478141230 test cost=1.484900213\n",
      "Epoch: 0006 train cost=1.475489794 test cost=1.484098081\n",
      "Epoch: 0007 train cost=1.473881273 test cost=1.482593612\n",
      "Epoch: 0008 train cost=1.471833188 test cost=1.481582415\n",
      "Epoch: 0009 train cost=1.470682928 test cost=1.481288508\n",
      "Epoch: 0010 train cost=1.470501536 test cost=1.481520288\n",
      "Epoch: 0011 train cost=1.469389922 test cost=1.481111101\n",
      "Epoch: 0012 train cost=1.468386085 test cost=1.480540403\n",
      "Epoch: 0013 train cost=1.467927486 test cost=1.480505963\n",
      "Epoch: 0014 train cost=1.467917865 test cost=1.480059120\n",
      "Epoch: 0015 train cost=1.467357543 test cost=1.479884026\n",
      "Accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "## Deifing the optimizers and cost function and actually running the model\n",
    "pred_relu = multilayer_perceptron_relu(x, weights_relu, biases)\n",
    "\n",
    "## Defining the loss function\n",
    "cost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_relu, labels = y_act))\n",
    "\n",
    "## Defining the adam optimizer\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
    "\n",
    "## Running the model\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(data.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "\n",
    "            # We are usinng adam optimizer\n",
    "            _, c, w = sess.run([adam, cost_fn, weights_sig], feed_dict={x:batch_xs, y_act: batch_ys})\n",
    "            train_avg_cost = train_avg_cost + c / total_batch\n",
    "            c = sess.run(cost_fn, feed_dict={x: data.test.images, y_act:data.test.labels})\n",
    "            test_avg_cost = test_avg_cost + c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "       \n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    predictions = tf.equal(tf.argmax(pred_relu,1), tf.argmax(y_act,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: data.test.images, y_act: data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
